<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Decision Trees | RK</title>
    <link>http://localhost:4321/tags/decision-trees/</link>
      <atom:link href="http://localhost:4321/tags/decision-trees/index.xml" rel="self" type="application/rss+xml" />
    <description>Decision Trees</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 22 Apr 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>How do CART models work</title>
      <link>http://localhost:4321/tutorial/how-do-cart-models-work/</link>
      <pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/tutorial/how-do-cart-models-work/</guid>
      <description>


&lt;p style=&#34;font-size:15px&#34;&gt;
&lt;i&gt; Cover image credit: &lt;a style=&#34;background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &amp;quot;San Francisco&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Ubuntu, Roboto, Noto, &amp;quot;Segoe UI&amp;quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px&#34; href=&#34;https://unsplash.com/@elliottengelmann?utm_medium=referral&amp;amp;utm_campaign=photographer-credit&amp;amp;utm_content=creditBadge&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; title=&#34;Download free do whatever you want high-resolution photos from Elliott Engelmann&#34;&gt;&lt;span style=&#34;display:inline-block;padding:2px 3px&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; style=&#34;height:12px;width:auto;position:relative;vertical-align:middle;top:-2px;fill:white&#34; viewBox=&#34;0 0 32 32&#34;&gt;
&lt;title&gt;
unsplash-logo
&lt;/title&gt;
&lt;path d=&#34;M10 9V0h12v9H10zm12 5h10v18H0V14h10v9h12v-9z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;&lt;span style=&#34;display:inline-block;padding:2px 3px&#34;&gt;Elliott Engelmann&lt;/span&gt;&lt;/a&gt;&lt;/i&gt;
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# r import
library(tidyverse)
library(knitr)
library(kableExtra)
library(Rgraphviz)
library(rpart)
library(rattle)
library(reticulate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# python import
import pandas as pd
from sklearn import tree
import matplotlib.pyplot as plt&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-is-cart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is CART?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;lassification &lt;strong&gt;a&lt;/strong&gt;nd &lt;strong&gt;R&lt;/strong&gt;egression &lt;strong&gt;T&lt;/strong&gt;rees (or &lt;strong&gt;CART&lt;/strong&gt; for short) is a type of supervised machine learning algorithm &lt;a href=&#34;#id_1&#34;&gt;[1]&lt;/a&gt; that is mostly used in classification problems &lt;a href=&#34;#id_2&#34;&gt;[2]&lt;/a&gt;, but can be applied for regression problems &lt;a href=&#34;#id_3&#34;&gt;[3]&lt;/a&gt; as well. It can handle both categorical and numerical input variables.&lt;/p&gt;
&lt;p&gt;In this tutorial, we are going to go step-by-step and build a tree for the &lt;strong&gt;classification&lt;/strong&gt; problem. A way to solve the regression problem will be discussed at the end.&lt;/p&gt;
&lt;p&gt;Imagine that we have medical data about patients and we have two drugs (Drug A and Drug B) that work differently for each patient based on their health metrics. We have such data for 15 patients. Now, the 16th patient comes in and we want to assign a drug to him based on this information. You can load the sample data set &lt;a href=&#34;DecisionTree_SampleData.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;patients_df &amp;lt;- read_csv(&amp;quot;DecisionTree_SampleData.csv&amp;quot;)
kable(patients_df, format = &amp;quot;markdown&amp;quot;, caption = &amp;quot;&amp;lt;b&amp;gt;Patients Data&amp;lt;/b&amp;gt;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 1: &lt;/span&gt;&lt;b&gt;Patients Data&lt;/b&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Patient ID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BP&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Drug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p05&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p06&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p07&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p08&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p09&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p13&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;???&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our decision tree is going to look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/tutorial/how-do-cart-models-work/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In case you are wondering why is it called “a tree”, you have to look at it upside down:&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;trees.jpg&#34;&gt;&lt;/img&gt;
&lt;/center&gt;
&lt;p&gt;Imagine that all our data is located at the &lt;strong&gt;root node&lt;/strong&gt; of the tree. We want to split the data by most significant features (that will create new &lt;strong&gt;decision nodes&lt;/strong&gt;) until we reach the &lt;strong&gt;leaf nodes&lt;/strong&gt; with the most homogeneous target possible (ideally just one class is present in each leaf node).&lt;/p&gt;
&lt;p&gt;The question is how do we split the data at each node?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;information-gain-and-entropy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Information Gain and Entropy&lt;/h2&gt;
&lt;p&gt;The main metrics to use for dealing with classification problems when building a tree are Information Gain (IG) and Gini impurity. We are going to use IG for this example, however, Gini impurity will be also discussed at the end.&lt;/p&gt;
&lt;p&gt;IG is based on the concept of entropy &lt;a href=&#34;#id_4&#34;&gt;[4]&lt;/a&gt; and information content from information theory &lt;a href=&#34;#id_5&#34;&gt;[5]&lt;/a&gt;. First of all, let’s build an intuition behind the entropy. Simply saying, entropy is the measure of chaos or uncertainty in the data.&lt;/p&gt;
&lt;p&gt;Imagine that I asked you to play the following simple game: I have two coins, one coin is unfair &lt;span class=&#34;math inline&#34;&gt;\(\left( P(\text{Heads}) = \frac{4}{5} \right)\)&lt;/span&gt;, the other one is fair &lt;span class=&#34;math inline&#34;&gt;\(\left( P(\text{Heads}) = \frac{1}{2} \right)\)&lt;/span&gt;. Your task is to pick a coin, make a prediction of an outcome (either Heads or Tails) and flip it. If your prediction is correct, I will give you $1. Which coin would you choose?&lt;/p&gt;
&lt;p&gt;Well, I hope that you would choose the unfair coin. In this case, you are &lt;em&gt;more certain&lt;/em&gt; about the outcome of the coin flip (on average it will come up Heads in &lt;span class=&#34;math inline&#34;&gt;\(80\%\)&lt;/span&gt; of the time). For the fair coin, you have the highest level of uncertainty, meaning that Heads and Tails can occur equally likely. Probability mass functions for both coins look as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  p_{\text{unfair}} =
    \begin{cases}
      \frac{4}{5} &amp;amp; \text{Heads}\\
      \frac{1}{5} &amp;amp; \text{Tails}\\
    \end{cases}       
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  p_{\text{fair}} =
    \begin{cases}
      \frac{1}{2} &amp;amp; \text{Heads}\\
      \frac{1}{2} &amp;amp; \text{Tails}\\
    \end{cases}       
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can calculate the entropy using the formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(X) = - \sum_{i=1}^{n} p(x_i) \cdot log_2 \big( p(x_i) \big)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of all possible outcomes for random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(log_2\)&lt;/span&gt; is the logarithm with base 2 (&lt;span class=&#34;math inline&#34;&gt;\(log_2 2 = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(log_2 4 = 2\)&lt;/span&gt;, etc).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Entropy for the unfair coin:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(\text{unfair}) = - \left( \frac{4}{5} \cdot log_2 \frac{4}{5} + \frac{1}{5} \cdot log_2 \frac{1}{5} \right) \\ \approx 0.72\]&lt;/span&gt;
Entropy for the fair coin:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(\text{fair}) = - \left( \frac{1}{2} \cdot log_2 \frac{1}{2} + \frac{1}{2} \cdot log_2 \frac{1}{2} \right) \\ = 1\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;entropy &amp;lt;- function(p) {
  return(-sum(p*log(p, base = 2)))
}

p_unfair &amp;lt;- c(4/5, 1/5)
p_fair &amp;lt;- c(1/2, 1/2)

entropy(p_unfair)
## [1] 0.7219281
entropy(p_fair)
## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, entropy for the fair coin is higher, meaning that the level of uncertainty is also higher. Note that entropy cannot be negative and the minimum value is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; which means the highest level of certainty. For example, imagine the unfair coin with the probability &lt;span class=&#34;math inline&#34;&gt;\(P(\text{Heads}) = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(\text{Tails}) = 0\)&lt;/span&gt;. In such a way you are sure that the coin will come up Heads and entropy is equal to zero.&lt;/p&gt;
&lt;p&gt;Coming back to our decision tree problem we want to split out data into nodes that have reduced the level of uncertainty. Basic algorithm look as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For each node:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Choose a feature from the data set.&lt;/li&gt;
&lt;li&gt;Calculate the significance (e.g., IG) of that feature in the splitting of data.&lt;/li&gt;
&lt;li&gt;Repeat for each feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Split the data by the feature that is the most significant splitter (e.g., highest IG).&lt;/li&gt;
&lt;li&gt;Repeat until there are no features left.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;growing-a-tree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Growing a Tree&lt;/h2&gt;
&lt;p&gt;We are going to split the data on training (first 15 patients) and test (16th patient) sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_df &amp;lt;- patients_df[-16, ]
test_df &amp;lt;- patients_df[16, ]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;root-node&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Root Node&lt;/h3&gt;
&lt;p&gt;The initial entropy of the distribution of target variable (&lt;code&gt;Drug&lt;/code&gt;) is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(X) = -\left( p(A) \cdot log_2 p(A) +  p(B) \cdot log_2 p(B) \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(A)\)&lt;/span&gt; - probability of Drug A. Since there are 7 observations out of 15 with the Drug A, &lt;span class=&#34;math inline&#34;&gt;\(p(A) = \frac{7}{15} \approx 0.47\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(B)\)&lt;/span&gt; - probability of Drug B. Since there are 8 observations out of 15 with the Drug A, &lt;span class=&#34;math inline&#34;&gt;\(p(B) = \frac{8}{15} \approx 0.53\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(X) = -\left(0.47 \cdot log_2 0.47 +  0.53 \cdot log_2 0.53 \right) \\ \approx 0.997\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_df &amp;lt;- train_df %&amp;gt;% 
  group_by(Drug) %&amp;gt;% 
  summarise(n = n(),
            p = round(n() / dim(train_df)[1], 2))

kable(temp_df, format = &amp;quot;markdown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Drug&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.53&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;entropy(temp_df$p)
## [1] 0.9974016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the next step, we are going to split the data by &lt;em&gt;each&lt;/em&gt; feature (&lt;code&gt;Gender&lt;/code&gt;, &lt;code&gt;BMI&lt;/code&gt;, &lt;code&gt;BP&lt;/code&gt;), calculate the entropy, and check which split variant gives the highest IG.&lt;/p&gt;
&lt;div id=&#34;gender&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Gender&lt;/h4&gt;
&lt;p&gt;Let’s start with &lt;code&gt;Gender&lt;/code&gt;. We have two labels in the data set: &lt;code&gt;M&lt;/code&gt; and &lt;code&gt;F&lt;/code&gt;. After splitting the data we end up with the following proportions:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Gender Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Meaning that among 15 patients 6 men were assigned to drug A and 2 were assigned to drug B, whereas 1 woman was assigned to drug A and 6 women were assigned to drug B.&lt;/p&gt;
&lt;p&gt;Calculating the entropy for each gender label:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Gender Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(A)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(B)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Entropy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{8}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{2}{8}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.811&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.592&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To measure the uncertainty reduction after the split we are going to use IG:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\scriptsize \text{IG} = \text{Entropy before the split} - \text{Weighted entropy after the split}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy before the split is the entropy at the root in this case&lt;/li&gt;
&lt;li&gt;Weighted entropy is defined as entropy times the label proportion. For example, weighted entropy for male patients equals to:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Weighted Entropy (M)} = \frac{\text{# Male patients}}{\text{# All patients}} \cdot H(M)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Weighted Entropy (M)} = \frac{8}{15} \cdot 0.811 = 0.433\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Gender Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(A)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(B)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Entropy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weight&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weighted Entropy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{8}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{2}{8}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.811&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{8}{15}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.433&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.592&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{7}{15}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.276&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now we can calculate the IG:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{IG} = 0.997 - (0.433 + 0.276) = 0.288\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bmi&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;BMI&lt;/h4&gt;
&lt;p&gt;Repeat the previous steps for the &lt;code&gt;BMI&lt;/code&gt; feature. We have two labels: &lt;code&gt;normal&lt;/code&gt; and &lt;code&gt;not normal&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;BMI Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(A)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(B)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Entropy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weight&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weighted Entropy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{3}{6}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{3}{6}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{15}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{4}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{5}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.991&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{9}{15}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.595&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{IG} = 0.997 - (0.4 + 0.595) = 0.002\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;blood-pressure&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Blood Pressure&lt;/h4&gt;
&lt;p&gt;And the same for &lt;code&gt;BP&lt;/code&gt; column (labels are the same: &lt;code&gt;normal&lt;/code&gt; and &lt;code&gt;not normal&lt;/code&gt;):&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;BP Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(A)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(B)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Entropy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weight&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weighted Entropy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{0}{6}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{6}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{15}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{7}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{2}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.764&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{9}{15}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.459&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{IG} = 0.997 - (0 + 0.459) = 0.538\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;node-summary&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Node Summary&lt;/h4&gt;
&lt;p&gt;As we can see &lt;code&gt;BP&lt;/code&gt; gives the highest IG, or in other words, if we split the data by this column we will be more certain about the target variable distribution. So our first split will look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/tutorial/how-do-cart-models-work/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bpnormal-node&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;BP:normal&lt;/code&gt; Node&lt;/h3&gt;
&lt;p&gt;Continue to grow the tree down from the &lt;code&gt;(normal node)&lt;/code&gt;. We filter observations that have &lt;code&gt;normal&lt;/code&gt; blood pressure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_df %&amp;gt;% 
  filter(BP == &amp;quot;normal&amp;quot;) %&amp;gt;%
  select(-BP) %&amp;gt;% 
  kable(format = &amp;quot;markdown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Patient ID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Drug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p06&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p08&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p13&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As we can see, there is no Drug A for this node and entropy is already 0. This node is going to be last for this branch with the drug B as an output with probability of 1. We can also think about this in a way that for all subjects with normal blood pressure drug B worked best.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/tutorial/how-do-cart-models-work/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bpnot-normal-node&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;BP:not normal&lt;/code&gt; Node&lt;/h3&gt;
&lt;p&gt;Filter all the observations that have &lt;code&gt;not normal&lt;/code&gt; blood pressure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_df %&amp;gt;% 
  filter(BP == &amp;quot;not normal&amp;quot;) %&amp;gt;% 
  select(-BP) %&amp;gt;% 
  kable(format = &amp;quot;markdown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Patient ID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Drug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p05&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p07&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p09&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now we consider this set as “initial set”. The entropy of target distribution at this node is &lt;span class=&#34;math inline&#34;&gt;\(0.764\)&lt;/span&gt; (as we have already calculated it). At this node we have two features: &lt;code&gt;Gender&lt;/code&gt; and &lt;code&gt;BMI&lt;/code&gt;. We perform the same procedure to find the best splitting feature.&lt;/p&gt;
&lt;div id=&#34;gender-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Gender&lt;/h4&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Gender Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(A)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(B)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Entropy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weight&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weighted Entropy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{6}{7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.592&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{7}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{2}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.222&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{IG} = 0.764 - (0.46 + 0.222) = 0.082\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bmi-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;BMI&lt;/h4&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;BMI Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(A)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(B)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Entropy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weight&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Weighted Entropy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{4}{5}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{5}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.722&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{5}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.401&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{3}{4}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{4}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.811&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{4}{9}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.361&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{IG} = 0.764 - (0.401 + 0.361) = 0.002\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;node-summary-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Node Summary&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Gender&lt;/code&gt; gives the highest IG after splitting the data. Now the tree will look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/tutorial/how-do-cart-models-work/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;genderm-node&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;Gender:M&lt;/code&gt; Node&lt;/h3&gt;
&lt;p&gt;Now we filer the set so that we have only observations with &lt;code&gt;BP:not normal&lt;/code&gt; and &lt;code&gt;Gender:M&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_df %&amp;gt;% 
  filter(BP == &amp;quot;not normal&amp;quot; &amp;amp; Gender == &amp;quot;M&amp;quot;) %&amp;gt;% 
  select(-c(BP, Gender)) %&amp;gt;% 
  kable(format = &amp;quot;markdown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Patient ID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Drug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p05&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p09&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There is just one feature left &lt;code&gt;BMI&lt;/code&gt;. Since we have no more features apart from this one we are going to “grow” last nodes with &lt;code&gt;BMI:normal&lt;/code&gt; and &lt;code&gt;BMI:not normal&lt;/code&gt; labels:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;BMI Label&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Drug B&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(A)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;P(B)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{2}{3}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{3}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{4}{4}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{0}{4}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For the &lt;code&gt;BMI:normal&lt;/code&gt; node the predicted class is going to be Drug A, since its probability is higher. The same applies to &lt;code&gt;BMI:not normal&lt;/code&gt; node.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/tutorial/how-do-cart-models-work/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genderf-node&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;Gender:F&lt;/code&gt; Node&lt;/h3&gt;
&lt;p&gt;Now we filer the set so that we have only observations with &lt;code&gt;BP:not normal&lt;/code&gt; and &lt;code&gt;Gender:F&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_df %&amp;gt;% 
  filter(BP == &amp;quot;not normal&amp;quot; &amp;amp; Gender == &amp;quot;F&amp;quot;) %&amp;gt;% 
  select(-c(BP, Gender)) %&amp;gt;% 
  kable(format = &amp;quot;markdown&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Patient ID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Drug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p07&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We are left with just 1 observation in each node. Predicted class for node &lt;code&gt;BMI:normal&lt;/code&gt; is going to be Drug A (&lt;span class=&#34;math inline&#34;&gt;\(P = 1.\)&lt;/span&gt;). Predicted class for node &lt;code&gt;BMI:not normal&lt;/code&gt; is going to be Drug B (&lt;span class=&#34;math inline&#34;&gt;\(P = 1.\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-tree&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final Tree&lt;/h3&gt;
&lt;p&gt;Given all the calculations we visualize the final tree:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;strong&gt;Code&lt;/strong&gt;
&lt;/summary&gt;
&lt;p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;node0 &amp;lt;- &amp;quot;Blood Pressure&amp;quot;
node1 &amp;lt;- &amp;quot;Drug B (P=1.)&amp;quot;
node2 &amp;lt;- &amp;quot;Gender&amp;quot;
node3 &amp;lt;- &amp;quot;BMI&amp;quot;
node4 &amp;lt;- &amp;quot;BMI &amp;quot;
node5 &amp;lt;- &amp;quot;Drug A (P=0.66)&amp;quot;
node6 &amp;lt;- &amp;quot;Drug A (P=1.)&amp;quot;
node7 &amp;lt;- &amp;quot;Drug A (P=1.) &amp;quot;
node8 &amp;lt;- &amp;quot;Drug B (P=1.) &amp;quot;
nodeNames &amp;lt;- c(node0, node1, node2, node3, 
               node4, node5, node6, node7, node8)
        
# create new graph object
rEG &amp;lt;- new(&amp;quot;graphNEL&amp;quot;, nodes=nodeNames, edgemode=&amp;quot;directed&amp;quot;)
rEG &amp;lt;- addEdge(node0, node1, rEG)
rEG &amp;lt;- addEdge(node0, node2, rEG)
rEG &amp;lt;- addEdge(node2, node3, rEG)
rEG &amp;lt;- addEdge(node2, node4, rEG)
rEG &amp;lt;- addEdge(node3, node5, rEG)
rEG &amp;lt;- addEdge(node3, node6, rEG)
rEG &amp;lt;- addEdge(node4, node7, rEG)
rEG &amp;lt;- addEdge(node4, node8, rEG)

plot(rEG, attrs = at)
text(150, 300, &amp;quot;normal&amp;quot;, col=&amp;quot;black&amp;quot;)
text(260, 300, &amp;quot;not normal&amp;quot;, col=&amp;quot;black&amp;quot;)
text(200, 190, &amp;quot;male&amp;quot;, col=&amp;quot;black&amp;quot;)
text(310, 190, &amp;quot;female&amp;quot;, col=&amp;quot;black&amp;quot;)
text(100, 80, &amp;quot;normal&amp;quot;, col=&amp;quot;black&amp;quot;)
text(230, 80, &amp;quot;not normal&amp;quot;, col=&amp;quot;black&amp;quot;)
text(290, 80, &amp;quot;normal&amp;quot;, col=&amp;quot;black&amp;quot;)
text(420, 80, &amp;quot;not normal&amp;quot;, col=&amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/tutorial/how-do-cart-models-work/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;Let’s look at the 16th patient:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_df %&amp;gt;% 
  kable(format = &amp;quot;markdown&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Patient ID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;BP&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Drug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not normal&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;???&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To make a prediction we just need to move top to bottom by the branches of our tree.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check &lt;code&gt;BP&lt;/code&gt;. Since &lt;code&gt;BP:not normal&lt;/code&gt;, go to the right branch.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gender:M&lt;/code&gt; =&amp;gt; left branch.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BMI:not normal&lt;/code&gt; =&amp;gt; right branch.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The predicted value is going to be &lt;code&gt;Drug A&lt;/code&gt; with a probability of &lt;span class=&#34;math inline&#34;&gt;\(1.0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;let-computer-do-the-math&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let Computer Do the Math&lt;/h2&gt;
&lt;div id=&#34;r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;p&gt;We can build a decision tree using &lt;code&gt;rpart&lt;/code&gt; function (from &lt;code&gt;rpart&lt;/code&gt; package) and plot it with the help of &lt;code&gt;fancyRpartPlot&lt;/code&gt; function (from &lt;code&gt;rattle&lt;/code&gt; package). Note, that R doesn’t require the categorical variables encoding since it can handle them by itself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_dt &amp;lt;- rpart(
  formula = Drug ~ Gender + BMI + BP,
  data = train_df,
  method = &amp;quot;class&amp;quot;,
  minsplit = 1,
  minbucket = 1,
  parms = list(
    split = &amp;quot;information&amp;quot; # for Gini impurity change to split = &amp;quot;gini&amp;quot;
  )
)

#plot decision tree model
fancyRpartPlot(model_dt, caption = NULL, type=4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/tutorial/how-do-cart-models-work/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Get a prediction for the 16th patient:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# return the class
predict(object = model_dt, newdata = test_df, type = &amp;quot;class&amp;quot;)
##      1 
## Drug A 
## Levels: Drug A Drug B

# return the probability of each class
predict(object = model_dt, newdata = test_df, type = &amp;quot;prob&amp;quot;)
##      Drug A    Drug B
## 1 0.8571429 0.1428571&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since R didn’t split &lt;code&gt;Gender:M&lt;/code&gt; node by &lt;code&gt;BMI&lt;/code&gt; the probabilities are quite different that we expected but it doesn’t effect the predicted class.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;p&gt;Unlike R, Python doesn’t allow categorical input variables, so we need to encode them:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;patients_df = pd.read_csv(&amp;quot;DecisionTree_SampleData.csv&amp;quot;)
# drop id column
patients_df.drop(&amp;#39;Patient ID&amp;#39;, axis=1, inplace=True)

# convert to dummy variables
patients_df_labeled = pd.get_dummies(patients_df)
columns_to_drop = [&amp;#39;Gender_F&amp;#39;, &amp;#39;BMI_not normal&amp;#39;, &amp;#39;BP_not normal&amp;#39;, &amp;#39;Drug_???&amp;#39;, &amp;#39;Drug_Drug B&amp;#39;]
patients_df_labeled.drop(
    columns_to_drop, 
    axis=1, 
    inplace=True)
unknown_drug = patients_df_labeled.iloc[15, :]
patients_df_labeled.drop(15, axis=0, inplace=True) # unknow drug row
patients_df_labeled.head().to_html(classes=&amp;#39;table table-striped&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;table border=&#34;0&#34; class=&#34;dataframe table table-striped&#34;&gt;

&lt;thead&gt;

&lt;tr style=&#34;text-align: right;&#34;&gt;

&lt;th&gt;
&lt;/th&gt;

&lt;th&gt;
Gender_M
&lt;/th&gt;

&lt;th&gt;
BMI_normal
&lt;/th&gt;

&lt;th&gt;
BP_normal
&lt;/th&gt;

&lt;th&gt;
Drug_Drug A
&lt;/th&gt;

&lt;/tr&gt;

&lt;/thead&gt;

&lt;tbody&gt;

&lt;tr&gt;

&lt;th&gt;
0
&lt;/th&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

&lt;th&gt;
1
&lt;/th&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

&lt;th&gt;
2
&lt;/th&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

&lt;th&gt;
3
&lt;/th&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

&lt;th&gt;
4
&lt;/th&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
0
&lt;/td&gt;

&lt;td&gt;
1
&lt;/td&gt;

&lt;/tr&gt;

&lt;/tbody&gt;

&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;For example &lt;code&gt;Gender_M:1&lt;/code&gt; means that subject is Male and &lt;code&gt;Gender_M:0&lt;/code&gt; means that subject is female.&lt;/p&gt;
&lt;p&gt;To build a decision tree model we can use a &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; function from &lt;code&gt;sklearn.tree&lt;/code&gt; module.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# split the data 
X = patients_df_labeled.drop(&amp;#39;Drug_Drug A&amp;#39;, axis=1)
y = patients_df_labeled[&amp;#39;Drug_Drug A&amp;#39;]

# fit the decision tree model
model = tree.DecisionTreeClassifier(
  criterion=&amp;#39;entropy&amp;#39; # for Gini impurity change to criterion=&amp;quot;gini&amp;quot;
)
model.fit(X, y)&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter&#39;s `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: &#34;&#34;;
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: &#34;▸&#34;;
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: &#34;▾&#34;;
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. &#34;i&#34; and &#34;?&#34;) */

/* Common style for &#34;i&#34; and &#34;?&#34; */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* &#34;?&#34;-specific style due to the `&lt;a&gt;` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
&lt;/style&gt;&lt;div id=&#34;sk-container-id-1&#34; class=&#34;sk-top-container&#34;&gt;&lt;div class=&#34;sk-text-repr-fallback&#34;&gt;&lt;pre&gt;DecisionTreeClassifier(criterion=&amp;#x27;entropy&amp;#x27;)&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class=&#34;sk-container&#34; hidden&gt;&lt;div class=&#34;sk-item&#34;&gt;&lt;div class=&#34;sk-estimator fitted sk-toggleable&#34;&gt;&lt;input class=&#34;sk-toggleable__control sk-hidden--visually&#34; id=&#34;sk-estimator-id-1&#34; type=&#34;checkbox&#34; checked&gt;&lt;label for=&#34;sk-estimator-id-1&#34; class=&#34;sk-toggleable__label fitted sk-toggleable__label-arrow fitted&#34;&gt;&amp;nbsp;&amp;nbsp;DecisionTreeClassifier&lt;a class=&#34;sk-estimator-doc-link fitted&#34; rel=&#34;noreferrer&#34; target=&#34;_blank&#34; href=&#34;https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html&#34;&gt;?&lt;span&gt;Documentation for DecisionTreeClassifier&lt;/span&gt;&lt;/a&gt;&lt;span class=&#34;sk-estimator-doc-link fitted&#34;&gt;i&lt;span&gt;Fitted&lt;/span&gt;&lt;/span&gt;&lt;/label&gt;&lt;div class=&#34;sk-toggleable__content fitted&#34;&gt;&lt;pre&gt;DecisionTreeClassifier(criterion=&amp;#x27;entropy&amp;#x27;)&lt;/pre&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# plot the tree
plt.figure(figsize=(10,10))
tree.plot_tree(
  model, filled=True, rounded=True,
  feature_names=X.columns.tolist(), proportion=False,
  class_names=[&amp;#39;Drug B&amp;#39;, &amp;#39;Drug A&amp;#39;]
)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;python-tree.png&#34;&gt;&lt;/img&gt;
&lt;/center&gt;
&lt;p&gt;Get a prediction for the 16th patient:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;unknown_drug = pd.DataFrame(unknown_drug).T
unknown_drug.drop([&amp;#39;Drug_Drug A&amp;#39;], axis=1, inplace=True)

# return the class
model.predict(unknown_drug)
## array([1], dtype=uint8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# return the probability of each class
model.predict_proba(unknown_drug)
## array([[0., 1.]])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-explanatory-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numerical Explanatory Variables&lt;/h2&gt;
&lt;p&gt;In our sample data set, we had only categorical variables (with 2 labels each). But what would the model do, if we had numerical value as an input?&lt;/p&gt;
&lt;p&gt;Let’s look again on the Python decision tree and its decision node conditions, for example, &lt;code&gt;Gender_M &amp;lt;= 0.5&lt;/code&gt;. Where does &lt;code&gt;0.5&lt;/code&gt; come from? The answer overlaps with the answer on how the model handles numerical input values.&lt;/p&gt;
&lt;p&gt;Recall the &lt;code&gt;Gender_M&lt;/code&gt; column we created for the Python train set. It consisted of two &lt;em&gt;numerical&lt;/em&gt; values (&lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;). What the model does is filter the data by the average value among these two numbers. Average value was: &lt;span class=&#34;math inline&#34;&gt;\(\frac{0+1}{2}=0.5\)&lt;/span&gt;. Now model has two sets of data: one set that matches the condition &lt;code&gt;Gender_M &amp;lt;= 0.5&lt;/code&gt; (&lt;code&gt;Gender&lt;/code&gt; is female) and the other set that doesn’t (&lt;code&gt;Gender&lt;/code&gt; is male).&lt;/p&gt;
&lt;p&gt;Imagine that we have added a third category &lt;code&gt;Non-binary&lt;/code&gt; to the &lt;code&gt;Gender&lt;/code&gt; feature. First, we would encode the values and create a new column &lt;code&gt;Gender_encoded&lt;/code&gt;, which would have three numerical values (&lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt;). Say, &lt;code&gt;Non-binary = 0&lt;/code&gt;, &lt;code&gt;Male = 1&lt;/code&gt;, &lt;code&gt;Female = 2&lt;/code&gt;. Now model would have two conditions for filtering the data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Gender_encoded &amp;lt;= 0.5&lt;/code&gt; (&lt;code&gt;Gender&lt;/code&gt; is non-binary)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gender_encoded &amp;lt;= 1.5&lt;/code&gt; (&lt;code&gt;Gender&lt;/code&gt; is non-binary OR male)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a general way, we can say that if we have a numerical feature, the algorithm will convert it to “categorical” by choosing the average value as a threshold between actual input values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filter 1: feature &amp;lt;= threshold&lt;/li&gt;
&lt;li&gt;filter 2: feature &amp;gt; threshold&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;blood_pressure&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;drug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;p01&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;p02&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;120&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;p03&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;150&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;p03&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Threshold values are: &lt;code&gt;110&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{100+120}{2} \right)\)&lt;/span&gt;, &lt;code&gt;135&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{120+150}{2} \right)\)&lt;/span&gt;, &lt;code&gt;175&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{150+200}{2} \right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-splitting-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Splitting Metrics&lt;/h2&gt;
&lt;div id=&#34;gini-impurity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gini Impurity&lt;/h3&gt;
&lt;p&gt;As told before, for classification problems one can also use Gini impurity as a way to find the best splitting feature. The idea stays the same, but the only difference is that instead of the Entropy formula we are using the Gini formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[G(X) = 1 - \sum_{i=1}^{n} p(x_i)^2\]&lt;/span&gt;
After calculating the Gini impurity value for each feature, we would calculate the gain and choose the feature that provides the highest gain.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\scriptsize \text{Gain} = \text{Gini impurity before the split} - \text{Weighted Gini impurity after the split}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-reduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance Reduction&lt;/h3&gt;
&lt;p&gt;If we are dealing with the regression problem (predicting numerical variable, rather than a class) we would use the variance reduction. Instead of calculating the probabilities of the target variable we calculate its variance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(x) = \frac{\sum_{i=1}^n(x_i- \bar x)^2}{n-1}\]&lt;/span&gt;
And in the same way we are looking for a variable that has the highest variance reduction, or in other words variance in the target variable becomes lower after the split.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\scriptsize \text{Variance Reduction} = \text{Variance before the split} - \text{Weighted variance after the split}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;We have looked at the example of the CART model for the binary classification problem. It is a relatively simple algorithm that doesn’t require much feature engineering. With the small data set and lots of free time, one can even build a model with “pen and paper”. Compared to other ML algorithms, CART can be considered as a “weak learner” due to the overfitting issue. However, they are often used as a base estimator in more complex bagging and boosting models (for example, Random Forest algorithm). Additionally, CART can help with the hidden data patterns, since they can be visualized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a name=&#34;1&#34;&gt;[1]:&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Supervised_learning&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Supervised_learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&#34;2&#34;&gt;[2]:&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_classification&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Statistical_classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&#34;3&#34;&gt;[3]:&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_analysis&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Regression_analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&#34;4&#34;&gt;[4]:&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Entropy_(information_theory)&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Entropy_(information_theory)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&#34;5&#34;&gt;[5]:&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_theory&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Information_theory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
